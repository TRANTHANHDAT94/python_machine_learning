{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5869783-46cd-4ef5-8d80-b1c6eaf559bd",
   "metadata": {},
   "source": [
    "# Lecture 09 - Nature Language Processing (NLP) & Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372869b4-6785-4aaa-8abc-3e29d875fb0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Definition - Định nghĩa\n",
    "- Sử dung phân tích làm sách sữ liệu kết hợp mô hình machine leaning để khai kthác thông tin trong dữ liệu ngôn ngữ\n",
    "Dữ liệu ngôn ngữ bao gồm \n",
    "- Nói cho nhau nghe\n",
    "- Viết cho nhau đọc\n",
    "\n",
    "\n",
    "- Kỹ thuật xử lý sữ liệu văn bản và khai thác thông tin:Textmining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ff90f-7126-4e50-8272-e7bb5ebc5092",
   "metadata": {},
   "source": [
    "## Flow - Quy trình\n",
    "1. Preprocessing: Clean and Normallize \n",
    "2. Tokenlizing: Tách từ\n",
    "3. Vectorizing\n",
    "4. Moddeliing\n",
    "5. Interpreting results and application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aab243-6557-4d52-8327-c3b96dbcda2e",
   "metadata": {},
   "source": [
    "## Application - Ứng dụng\n",
    "1. Sentimental Analysis: Phân tích cảm tính Cho mấy sao?? Nhưng khôg biết rõ ràng. Dùng phân tích chứng khoán Twitter and Facebook \n",
    "2. Search suggestion:\n",
    "3. Speech recognition \n",
    "4. etc more and more "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5b914-9acd-4e9f-9072-a6663fea3313",
   "metadata": {},
   "source": [
    "## Tools & Library\n",
    "\n",
    "1. Python Nature Language Toolkit (Python NLTK) # Để xử lý dữ liệu dạng text cho mô hình texmining \n",
    "2. Gensim/Tensorflow etc. # Để xây dựng mô hình text minning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451b6a6e-35dc-4f5d-bb6b-6c4af1f79c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement string (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for string\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ca5f91-634d-4963-b8a8-4241501d1994",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'string' has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ad0e51e8fa9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'string' has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import string \n",
    "string.split()\n",
    "string.strim()\n",
    "string.lower()\n",
    "string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21753bb-7fb5-4df5-b912-06b6bcfc1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cách mạng 1 từ \n",
    "#a an the in out từ đệm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0437f3-5113-472a-ab06-01f3999c4002",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe2646-68db-485a-a3d7-9c5c3e3c2bb5",
   "metadata": {},
   "source": [
    "### Token hóa dữ liệu text (Text tokenization)\n",
    "\n",
    "Là quá trình làm sạch và biến đổi dữ liệu text thành dạng sẵn sàng đưa vào mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8ab72-48f2-4e8d-b889-1f8637c81797",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu text (Text preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80020240-0711-4916-b200-3b76dc1e096f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RangeIndex(start=0, stop=876, step=1),\n",
       " Index(['title', 'snippet', 'content'], dtype='object'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_json('data.json')\n",
    "data.index, data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6dc2014-8638-4855-bd29-fda371ddc1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>(Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...</td>\n",
       "      <td>Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...</td>\n",
       "      <td>Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...</td>\n",
       "      <td>Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đề xuất tăng liều lượng gói kích thích kinh tế...</td>\n",
       "      <td>Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...</td>\n",
       "      <td>Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tác động từ chính sách tín dụng: BĐS có thể đó...</td>\n",
       "      <td>Động thái thắt chặt hay nới lỏng của chính sác...</td>\n",
       "      <td>Lời tòa soạn Thị trường bất động sản ngày càng...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "1  Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...   \n",
       "2  Đề xuất tăng liều lượng gói kích thích kinh tế...   \n",
       "3  Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "4  Tác động từ chính sách tín dụng: BĐS có thể đó...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  (Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...   \n",
       "1  Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...   \n",
       "2  Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...   \n",
       "3  Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...   \n",
       "4  Động thái thắt chặt hay nới lỏng của chính sác...   \n",
       "\n",
       "                                             content  \n",
       "0  Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...  \n",
       "1  Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...  \n",
       "2  Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...  \n",
       "3  Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...  \n",
       "4  Lời tòa soạn Thị trường bất động sản ngày càng...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02cb0967-d219-44f7-9793-ba1cf8b0bed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thống nhất kịch bản tăng trưởng 3-4%, lạm phát dưới 4%'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "651749cb-7a5f-4843-b816-67b5a34effde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Làm sạch dữ liệu chỉ giữ lại dữ liệu bằng chữ\n",
    "# 1. Nếu là dữ liệu scrapped từ web thì chỉ trích lọc dữ liệu chứ (text only)\n",
    "# 2. Loại bỏ hyperlink (nếu có) #https://baophapluat.com ->baophapluat lẫn\n",
    "# 3. Nếu là dữ liệu web thì cần loại bỏ emoji(example: :smile: :angry:\n",
    "# 4. Loại bỏ tất cả các dấu (.,\\/!@#$%^&*()+_ etc.)\n",
    "# 5. Loại bỏ tất cả các số\n",
    "# 6. Loại bỏ các khoảng trắng và đổi text thành lowercase #normallize  T == t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "703cf364-898e-4494-8640-a16d3cea1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5959931b-a401-4a4b-9cc9-fd43e996ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. lấy text bằng beautifulshop trong dataEngineering phần database\n",
    "import bs4 \n",
    "def del_html(text):\n",
    "    soup = bs4.BeautifulSoup(text)\n",
    "    return soup.get_text(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c1e331-1dcf-402b-8af8-b36629a639c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Loại bỏ hyperlink remove hyperlink\n",
    "import re # regular expression\n",
    "def del_link(text):\n",
    "    link = r'http[\\S]*' #pattern quy tắc * thay cho tất cả những thằng còn lại bất kì\n",
    "    text = re.sub(link, ' ', str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4dee2e0-604d-4c64-ae3e-84cfb8eb648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04be7513-0bb3-4e8f-b7e6-3f517d5a8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Loai bỏ tất cả emoji bằng kí tự trống\n",
    "import demoji\n",
    "def del_emoji(text):\n",
    "    return demoji.replace(text, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b3915e9-e245-453d-9f89-ba460d5dbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Loại bỏ toàn bộ dấu\n",
    "def del_punctuation(doc):\n",
    "    pattern = r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\“\\”\\\\\\-\\:…&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\•]'\n",
    "    record = re.sub(pattern, ' ', doc)\n",
    "    return re.sub(r'\\n', ' ', record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa243430-1dfd-4556-b5a8-5d762c3e4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. d+ thay thế cho các kí tự là chữ số 0,1,2,3,4,...789\n",
    "def del_numbers(text):\n",
    "    return re.sub(r'\\d+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4f2d156-59e4-4c3c-a266-f768aa33c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Loại bỏ khoảng trắng bằng (dấu cách , Tab, xuống dòng) ==> từ 1 khoảng trắng đứng cạnh nhau trở lên s+ trừ đấu cách đứng 1 mình\n",
    "def del_space(doc):\n",
    "    space_pattern = r'\\s+'\n",
    "    return re.sub(space_pattern, ' ', doc.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba04a33a-6446-42c9-95ff-11436e2c143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 \n",
    "import re # regular expression\n",
    "import demoji\n",
    "def clean_documents(text):\n",
    "    def del_numbers(text):\n",
    "        return re.sub(r'\\d+', ' ', text)\n",
    "    def del_punctuation(doc):\n",
    "        pattern = r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\“\\”\\\\\\-\\:…&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\•]'\n",
    "        record = re.sub(pattern, ' ', doc)\n",
    "        return re.sub(r'\\n', ' ', record)\n",
    "    def del_emoji(text):\n",
    "        return demoji.replace(text, '')\n",
    "    def del_link(text):\n",
    "        link = r'http[\\S]*' #pattern quy tắc * thay cho tất cả những thằng còn lại bất kì\n",
    "        text = re.sub(link, ' ', str(text))\n",
    "        return text\n",
    "    def del_html(text):\n",
    "        soup = bs4.BeautifulSoup(text)\n",
    "        return soup.get_text(' ')\n",
    "    def del_space(doc):\n",
    "        space_pattern = r'\\s+'\n",
    "        return re.sub(space_pattern, ' ', doc.lower())\n",
    "    for func in [del_html, del_link, del_emoji, del_punctuation, del_numbers, del_space]:\n",
    "        text = func(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c281743-b3c3-4119-a0da-e99c8cc988fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn\n",
      "del_html : Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn\n",
      "del_link : Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn\n",
      "del_emoji : Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn\n",
      "del_punctuation : Thủ tướng  Khác với đa số các nước  dư địa chính sách tài khóa  tiền tệ của Việt Nam còn khá lớn\n",
      "del_numbers : Thủ tướng  Khác với đa số các nước  dư địa chính sách tài khóa  tiền tệ của Việt Nam còn khá lớn\n",
      "del_space : thủ tướng khác với đa số các nước dư địa chính sách tài khóa tiền tệ của việt nam còn khá lớn\n"
     ]
    }
   ],
   "source": [
    "text = data.iloc[0,0]\n",
    "print(text)\n",
    "for func in [del_html, del_link, del_emoji, del_punctuation, \n",
    "             del_numbers, del_space]:\n",
    "    text = func(text)\n",
    "    print(func.__name__, ':', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4e134d3-8f49-47c5-841f-214d9b7c808c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thủ tướng khác với đa số các nước dư địa chính sách tài khóa tiền tệ của việt nam còn khá lớn'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data.iloc[0,0]\n",
    "text =clean_documents(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c807e-c950-44b4-923a-3da9c08f74e8",
   "metadata": {},
   "source": [
    "#### Chia từ (Word tokenizing) - Tiếng Anh vs Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c985a9ba-9a19-4e02-9b09-e99c24578207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thủ',\n",
       " 'tướng',\n",
       " 'khác',\n",
       " 'với',\n",
       " 'đa',\n",
       " 'số',\n",
       " 'các',\n",
       " 'nước',\n",
       " 'dư',\n",
       " 'địa',\n",
       " 'chính',\n",
       " 'sách',\n",
       " 'tài',\n",
       " 'khóa',\n",
       " 'tiền',\n",
       " 'tệ',\n",
       " 'của',\n",
       " 'việt',\n",
       " 'nam',\n",
       " 'còn',\n",
       " 'khá',\n",
       " 'lớn']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tiếng Anh\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3377d6c5-7b47-4a12-9dd4-9b5385f07928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from nltk) (2021.8.21)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from click->nltk) (3.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk #natural laguage toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "734b902e-accd-4b91-84ae-86700ca78c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e7421b75-c2cf-407c-8506-a1ba5a31d44b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog', '.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = 'A quick brown fox jump over a lazy dog.'\n",
    "nltk.word_tokenize(sentence) # Tách thành yếu tố nhỏ nhất trong câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08e1184d-1a5f-4ec0-8621-cc3035cd3e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A quick brown fox jump over a lazy dog.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sentence) #sentences tokenize tách câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1e32561-efc2-4f71-9643-e2812fc0fcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog', '.'],\n",
       " ['He', 'is', 'the', 'royal', 'king', '.']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = '''A quick brown fox jump over a lazy dog.\n",
    "He is the royal king.\n",
    "'''\n",
    "\n",
    "[nltk.word_tokenize(i) for i in nltk.sent_tokenize(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b98afc68-e131-4c90-8000-5b1db2680003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog'],\n",
       " ['He', 'is', 'the', 'royal', 'king'],\n",
       " []]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent.split() for sent in sentences.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a27ba149-2dfd-409f-a23f-550018530fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thủ', 'tướng', 'khác', 'với', 'đa', 'số', 'các', 'nước', 'dư', 'địa', 'chính', 'sách', 'tài', 'khóa', 'tiền', 'tệ', 'của', 'việt', 'nam', 'còn', 'khá', 'lớn']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea9033be-796a-42a6-a4ab-7fd5764c5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6fbfac9-9bab-4af3-b671-b64aad2476bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (0.1.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from pyvi) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from scikit-learn->pyvi) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from scikit-learn->pyvi) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from scikit-learn->pyvi) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from scikit-learn->pyvi) (1.19.5)\n",
      "Requirement already satisfied: tabulate in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from sklearn-crfsuite->pyvi) (4.62.1)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /opt/anaconda3/envs/anthony/lib/python3.7/site-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd019cd1-8530-4639-8868-5159e79cd2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thủ_tướng khác với đa_số các nước dư địa_chính_sách tài khóa tiền_tệ của việt nam còn khá lớn'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi.ViTokenizer import tokenize\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bd0bb65-92a9-429a-8f4c-8c6c074f8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thủ_tướng', 'khác', 'với', 'đa_số', 'các', 'nước', 'dư', 'địa_chính_sách', 'tài', 'khóa', 'tiền_tệ', 'của', 'việt', 'nam', 'còn', 'khá', 'lớn']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(text).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9ba76-913e-4803-a39b-d6b537ed57bb",
   "metadata": {},
   "source": [
    "#### Put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c038b40f-907a-4cea-bd75-9ff06581b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = del_html(text)\n",
    "    text = del_link(text)\n",
    "    text = del_numbers(text)\n",
    "    text = del_emoji(text)\n",
    "    text = del_punctuation(text)\n",
    "    text = del_space(text)\n",
    "    return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98d08d23-2ebd-44e8-be76-6b7866777708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Thủ tướng: Khác với đa số các nước, dư địa chí...\n",
       "1      Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...\n",
       "2      Đề xuất tăng liều lượng gói kích thích kinh tế...\n",
       "3      Thủ tướng: Khác với đa số các nước, dư địa chí...\n",
       "4      Tác động từ chính sách tín dụng: BĐS có thể đó...\n",
       "                             ...                        \n",
       "871    Việt Nam thành lập Viện nghiên cứu phát triển ...\n",
       "872    Bất động sản chờ lực bật mới Chịu nhiều khó kh...\n",
       "873    Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...\n",
       "874    Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...\n",
       "875    Sẵn sàng đón nhận dòng vốn đầu tư dịch chuyển ...\n",
       "Name: corpus, Length: 876, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data.apply(lambda x: ' '.join(x), axis=1) #gộp dữ liệu text từ 3 cột của dataframe\n",
    "data['corpus'] #in text mining the combination of all text is corpus/corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "664ac7b1-82d1-400c-bb94-0746b098a2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      thủ_tướng khác với đa_số các nước dư địa_chính...\n",
       "1      thống_nhất kịch_bản tăng_trưởng lạm_phát dưới ...\n",
       "2      đề_xuất tăng liều_lượng gói kích_thích kinh_tế...\n",
       "3      thủ_tướng khác với đa_số các nước dư địa_chính...\n",
       "4      tác_động từ chính_sách tín_dụng bđs có_thể đón...\n",
       "                             ...                        \n",
       "871    việt nam thành_lập viện nghiên_cứu phát_triển ...\n",
       "872    bất_động_sản chờ lực bật mới chịu nhiều khó_kh...\n",
       "873    diễn_đàn bất_động_sản cơ_hội mới từ chính_sách...\n",
       "874    thống_đốc lê minh hưng tín_dụng tăng trở_lại n...\n",
       "875    sẵn_sàng đón_nhận dòng vốn đầu_tư dịch_chuyển ...\n",
       "Name: corpus, Length: 876, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data['corpus'].apply(clean_text)\n",
    "data['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df990f68-245c-41ed-8530-3c92e0947699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thủ_tướng khác với đa_số các nước dư địa_chính_sách tài khóa tiền_tệ của việt nam còn khá lớn tổ_quốc với bối_cảnh hiện_nay hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia thống_nhất kịch_bản tăng_trưởng từ kiểm_soát lạm_phát dưới năm và đầu tăng_trưởng tín_dụng trên chủ_trương tăng thêm bội_chi ngân_sách nợ công khoảng gdp sáng ngày hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia đã họp dưới sự chủ_trì của thủ_tướng nguyễn xuân phúc chủ_tịch hội_đồng dư địa_chính_sách tài khóa tiền_'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0, 'corpus'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa513f1e-0125-4cdc-a268-883af940025b",
   "metadata": {},
   "source": [
    "### Mã hóa dữ liệu text (Word embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d782c1-6302-4a8e-b8de-e936565beb95",
   "metadata": {},
   "source": [
    "#### 1. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76b015ad-aa33-49a1-9937-677dffac05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển thành dạng lowercase (nếu chưa)\n",
    "# tách từ (tokenize), loại bỏ từ trùng lặp (set)\n",
    "# Sắp xếp theo thứ tự a-z\n",
    "# Lấy giá trị vị trí\n",
    "# Chuyển đổi thành vector [0, 1] tương ứng giá trị vị trí\n",
    "#our moddelling just can use the number therefore we have to move it(transform0 to the number dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e93876c-d23f-4e3b-b6f0-8e21ee77c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "['can', 'eat', 'i', 'pizza', 'the']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Can I eat the pizza'\n",
    "arr = sorted(nltk.word_tokenize(sentence.lower())) #Tácch tokenize and sort từ a->z\n",
    "print(list(range(len(arr))))\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97cd81d8-3d7c-4aa6-86b9-45cdfa7eb8e1",
   "metadata": {},
   "source": [
    "[[1. 0. 0. 0. 0.] # can\n",
    " [0. 0. 1. 0. 0.] # i\n",
    " [0. 1. 0. 0. 0.] # eat\n",
    " [0. 0. 0. 0. 1.] # the\n",
    " [0. 0. 0. 1. 0.]] # pizza\n",
    "#Calss: [0, 1, 2, 3, 4] and ['can', 'eat', 'i', 'pizza', 'the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b38888df-bbca-4618-9546-b49bc1be16e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "values:  ['can', 'eat', 'i', 'pizza', 'the']\n",
      "\n",
      "integer encoded:  [0, 2, 1, 4, 3]\n",
      "\n",
      "MATRIX:\n",
      "[[1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "docs = \"Can I eat the Pizza\".lower().split()\n",
    "doc1 = set(docs)\n",
    "doc1 = sorted(doc1)\n",
    "print (\"\\nvalues: \", doc1)\n",
    "\n",
    "integer_encoded = []\n",
    "for i in docs:\n",
    "    v = np.where( np.array(doc1) == i)[0][0]\n",
    "    integer_encoded.append(v)\n",
    "print (\"\\ninteger encoded: \",integer_encoded)\n",
    "\n",
    "def get_vec(len_doc,word):\n",
    "    empty_vector = [0] * len_doc\n",
    "    vect = 0\n",
    "    find = np.where( np.array(doc1) == word)[0][0]\n",
    "    empty_vector[find] = 1\n",
    "    return empty_vector\n",
    "\n",
    "def get_matrix(doc1):\n",
    "    mat = []\n",
    "    len_doc = len(doc1)\n",
    "    for i in docs:\n",
    "        vec = get_vec(len_doc,i)\n",
    "        mat.append(vec)\n",
    "        \n",
    "    return np.asarray(mat)\n",
    "\n",
    "print (\"\\nMATRIX:\")\n",
    "print (get_matrix(doc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b347065-59af-414d-8fee-f37389be5b5d",
   "metadata": {},
   "source": [
    "### Khai phá dữ liệu text (Text Mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82a86e-d2e8-4c4c-a418-124c90218d1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.Khái niệm Bag-of-Words\n",
    "\n",
    "Là kỹ thuật chia văn bản thành các tổ hợp từ khác nhau (bằng phương pháp tokenize). Cách chia phổ biến là mỗi câu thành 1 văn bản (bag) và mỗi văn bản được chia thành từ (word). Dựa vào đó có thể đo lường mức độ xuất hiện của các từ trong văn bản và xây dựng mối liên hệ giữa ngữ cảnh và các từ. \n",
    "\n",
    "Hai yếu tố:\n",
    "1. Từ điển của các từ được sử dụng\n",
    "2. Mức độ xuất hiện của các từ trong từ điển\n",
    "*Mỗi từ hay token được gọi là một `gram`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2f157ea-c709-404b-a67c-fb87442d5f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was the best of times',\n",
       " 'It was the worst of times',\n",
       " 'It was the age of wisdom',\n",
       " 'It was the age of foolishness']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'It was the best of times',\n",
    "    'It was the worst of times',\n",
    "    'It was the age of wisdom',\n",
    "    'It was the age of foolishness'\n",
    "]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0d8ef21-fcb3-48fa-b7fd-fceacb27a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['It', 'was', 'the', 'best', 'of', 'times', 'worst', 'age', 'wisdom', 'foolishness'] #Vector từ điển"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ca23669-38fe-4bd4-88d9-d9b2f05d334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It was the best of times :\t [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "#It was the worst of times :\t [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "#It was the age of wisdom :\t [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "#It was the age of foolishness :\t [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbea88b0-c557-4e32-92bf-d9ff4d024937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', 'worst', 'age', 'wisdom', 'foolishness']\n"
     ]
    }
   ],
   "source": [
    "dictionary = []                          #Thêm dần vào từ điển\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    for w in words:\n",
    "        if w not in dictionary:\n",
    "            dictionary.append(w)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "125e3c08-234f-4810-a497-c50e7c531ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times :\t [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "It was the worst of times :\t [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "It was the age of wisdom :\t [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "It was the age of foolishness :\t [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    vec = [1 if w in sent else 0 for w in dictionary]\n",
    "    print(sent, ':\\t', vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3",
   "metadata": {},
   "source": [
    "#### 3.TF-IDF (Term frequency-Inverse Document frequency)\n",
    "\n",
    "Ngữ cảnh: I am very angry ->\"very angry\" Tập trung vào cách từ mang nhiều thông tin\n",
    "\n",
    "Đơn vị đo thông tin trong machine learnning: entropy \n",
    "\n",
    "Đo lường tần suất ***hợp lý*** một từ (hay token) xuất hiện trong văn bản. Tần suất này được tính bằng: Mức độ xuất hiện của từ trong văn bản chia cho tỉ lệ văn bản mà từ đó xuất hiện trên tổng tất cả số lượng văn bản.\n",
    "\n",
    "Ví dụ: đối với như `what` hay `the`, các từ này xuất hiện rất nhiều tuy nhiên ko mang nhiều ý nghĩa nên cần có phương pháp loại trừ các từ này ra khỏi mô hình. Vì vậy ngoài tính toán mức độ xuất hiện của các từ trong văn bản, tuy nhiên nếu văn bản nào cũng xuất hiện từ này (hoặc đơn giản là rất nhiều > 90%) thì các từ này sẽ bị loại ra.\n",
    "\n",
    "$$ tf-idf(t, d, D)  = tf(t, d) \\dot idf(t, D)$$\n",
    "\n",
    "> `t`: từ (word hay token)\n",
    " \n",
    "> `d`: văn bản (document)\n",
    " \n",
    "> `D`: tệp các văn bản (documents)\n",
    "\n",
    "trong đó:\n",
    "\n",
    "$$ tf(t, d) = \\log(1 + freq(t, d)) $$\n",
    "$$ idf(t, D) = \\log \\left( \\dfrac{N}{count(d \\in D: t \\in d)} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d500e30-be68-45fe-8fcb-296cc4c460cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ccb54cd-d254-458b-9c7c-27bccb7e24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/anthony/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97fd43b9-a214-460b-96a2-a7e525f0df43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['age', 1], ['foolishness', 1], ['it', 1], ['of', 1], ['the', 1], ['was', 1]]\n",
      "[['age', 1], ['foolishness', 1], ['it', 1], ['of', 1], ['the', 1], ['was', 1]]\n",
      "[['age', 1], ['foolishness', 1], ['it', 1], ['of', 1], ['the', 1], ['was', 1]]\n",
      "[['age', 1], ['foolishness', 1], ['it', 1], ['of', 1], ['the', 1], ['was', 1]]\n",
      "[['age', 0.0], ['foolishness', 0.0], ['it', 0.0], ['of', 0.0], ['the', 0.0], ['was', 0.0]]\n",
      "[['age', 0.0], ['foolishness', 0.0], ['it', 0.0], ['of', 0.0], ['the', 0.0], ['was', 0.0]]\n",
      "[['age', 0.0], ['foolishness', 0.0], ['it', 0.0], ['of', 0.0], ['the', 0.0], ['was', 0.0]]\n",
      "[['age', 0.0], ['foolishness', 0.0], ['it', 0.0], ['of', 0.0], ['the', 0.0], ['was', 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "tokenized = [simple_preprocess(sent) for doc in sentences]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenized]\n",
    "\n",
    "for doc in BoW_corpus:\n",
    "    print([[dictionary[id], freq] for id, freq in doc])\n",
    "\n",
    "tfidf = models.TfidfModel(BoW_corpus, smartirs='ntc')\n",
    "for doc in tfidf[BoW_corpus]:\n",
    "    print([[dictionary[id], np.around(freq)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b11b6e9-3271-4c0b-b9b8-51709f57ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 'was', 'the', 'best', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'worst', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'wisdom'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'foolishness']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = [simple_preprocess(sent) for sent in sentences]\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb319f69-70ac-4e82-ac01-ded493dc4e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (5, 1), (7, 1), (9, 1)]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary()\n",
    "bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenized]\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d37112c-a377-4e54-a479-749d95fb1d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['best', 1], ['it', 1], ['of', 1], ['the', 1], ['times', 1], ['was', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['times', 1], ['was', 1], ['worst', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['was', 1], ['age', 1], ['wisdom', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['was', 1], ['age', 1], ['foolishness', 1]]\n"
     ]
    }
   ],
   "source": [
    "for doc in bow_corpus:\n",
    "    print([[dictionary[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c84a2c4c-d198-4445-8b01-9b12c23990f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x13f00f5f8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus, smartirs='ntc')\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25d55915-d15d-46b4-98a7-878a8d61118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['best', 0.8448462391634638], ['it', 0.11713529839512135], ['of', 0.11713529839512135], ['the', 0.11713529839512135], ['times', 0.48099076877929264], ['was', 0.11713529839512135]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['times', 0.48099076877929253], ['was', 0.11713529839512132], ['worst', 0.8448462391634637]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['was', 0.11713529839512132], ['age', 0.48099076877929253], ['wisdom', 0.8448462391634637]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['was', 0.11713529839512132], ['age', 0.48099076877929253], ['foolishness', 0.8448462391634637]]\n"
     ]
    }
   ],
   "source": [
    "for doc in tfidf[bow_corpus]:\n",
    "    print([[dictionary[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ada12a94-82d2-4973-8041-1e8583a55584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['biện_pháp', 0.041056865547833686],\n",
       " ['bán_lẻ', 0.04908556240345798],\n",
       " ['bên', 0.022150442731134225],\n",
       " ['bùi', 0.07118245725073921],\n",
       " ['bùng_phát', 0.04460812603722285],\n",
       " ['bảo_vệ', 0.04995756609294067],\n",
       " ['bối_cảnh', 0.034662574749689465],\n",
       " ['bội_chi', 0.1006266484130534],\n",
       " ['ca', 0.060608811632763285],\n",
       " ['chi', 0.04908556240345798]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary()\n",
    "tokenized = [doc.split(' ') for doc in data['corpus']]\n",
    "bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenized]\n",
    "\n",
    "model = models.TfidfModel(bow_corpus)\n",
    "[[dictionary[id], freq] for id, freq in model[bow_corpus[0]][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5e4bac1-8eb5-4bb6-a5e3-a883ea86d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biện_pháp'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc2036-7dc6-4b33-baac-7c1ab14fe285",
   "metadata": {},
   "source": [
    "#### Mô hình Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f5ac5-3024-4991-8a7b-27ec8002ac29",
   "metadata": {},
   "source": [
    "*\"Word2Vec was developed at Google by Tomas Mikolov, et al. and uses Neural Networks to learn word embeddings. The beauty with word2vec is that the vectors are learned by understanding the context in which words appear. The result is vectors in which words with similar meanings end up with a similar numerical representation.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe00fd3-483f-4343-b331-8bdbde4d8d03",
   "metadata": {},
   "source": [
    "**One-hot-encoding**\n",
    "\n",
    "All word are treated equal\n",
    "\n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/b3c56245-db43-48ab-b652-9ba03f4d9900.jpg?ssl=1)\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "Word with similar numeric value are similar in meaning\n",
    "\n",
    "![](https://i1.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/8cbfc874-3ba3-46c8-ab68-2711812ecbf1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e7484-2125-4bfa-8033-9f11341f6c93",
   "metadata": {},
   "source": [
    "Hai loại mô hình Word2Vec: **CBOW** (Continuous Bag-of-Word) và **Skip-Gram**\n",
    "\n",
    "Continuous Bag of Words (CBOW): *nhìn hình (ngữ cảnh) đoán chữ*\n",
    "\n",
    "Ngược lại, Skip-Gram: *nhìn chữ đoán hình (ngữ cảnh)*\n",
    "\n",
    "![](https://i0.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/7938152f-71c8-4f28-9c25-06735e6e2b67.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1e4d21d0-2f49-4f45-a365-5f0c2718df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41c90846-6e81-448f-b604-c797cab58b7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-d99549747ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 == skipgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(doc_tokenized, min_count=5, sg=0) # 1 == skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2baa14f-3319-4d69-887d-76adc73fcbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized[:1][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0289002-5b51-4884-8bb6-8cbaf18f8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.key_to_index\n",
    "print(pd.Series(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdfb20-169b-46c0-833f-0c3e1859d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['bùng_phát']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245edc78-bd0b-4642-b508-20c65552ea40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('bùng_phát')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aea233-3d03-4566-8876-153325d71cd9",
   "metadata": {},
   "source": [
    "**Skip-gram**\n",
    "\n",
    "Window Size defines how many words before and after the target word will be used as context, typically a Window Size is 5. \n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/a8066c1d-c532-4549-bb24-19dfea5eb178_med.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860004a0-d665-4d69-a4b6-41f7ef94ccea",
   "metadata": {},
   "source": [
    "Using a window size of 2 the input pairs for training on w(4) royal would be:\n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/training_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ee0db-98c0-4baa-84b4-9a08b988bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(doc_tokenized, min_count=5, sg=1) # 1 == skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e184d-7c60-4ed9-9f52-11fb709641d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.key_to_index\n",
    "print(pd.Series(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63630684-fb57-41cc-bf89-ccb47f16e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['bùng_phát']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1823d-a501-4b76-87c3-45e174b30ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('bùng_phát')\n",
    "sim_words\n",
    "#Showw the word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

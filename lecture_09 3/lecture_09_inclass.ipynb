{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5869783-46cd-4ef5-8d80-b1c6eaf559bd",
   "metadata": {},
   "source": [
    "# Lecture 09 - Nature Language Processing (NLP) & Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372869b4-6785-4aaa-8abc-3e29d875fb0c",
   "metadata": {},
   "source": [
    "## Definition - Định nghĩa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ff90f-7126-4e50-8272-e7bb5ebc5092",
   "metadata": {},
   "source": [
    "## Flow - Quy trình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aab243-6557-4d52-8327-c3b96dbcda2e",
   "metadata": {},
   "source": [
    "## Application - Ứng dụng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5b914-9acd-4e9f-9072-a6663fea3313",
   "metadata": {},
   "source": [
    "## Tools & Library\n",
    "\n",
    "1. Python Nature Language Toolkit (Python NLTK)\n",
    "2. Gensim/Tensorflow etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0437f3-5113-472a-ab06-01f3999c4002",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe2646-68db-485a-a3d7-9c5c3e3c2bb5",
   "metadata": {},
   "source": [
    "### Token hóa dữ liệu text (Text tokenization)\n",
    "\n",
    "Là quá trình làm sạch và biến đổi dữ liệu text thành dạng sẵn sàng đưa vào mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8ab72-48f2-4e8d-b889-1f8637c81797",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu text (Text preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80020240-0711-4916-b200-3b76dc1e096f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>(Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...</td>\n",
       "      <td>Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...</td>\n",
       "      <td>Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...</td>\n",
       "      <td>Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đề xuất tăng liều lượng gói kích thích kinh tế...</td>\n",
       "      <td>Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...</td>\n",
       "      <td>Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tác động từ chính sách tín dụng: BĐS có thể đó...</td>\n",
       "      <td>Động thái thắt chặt hay nới lỏng của chính sác...</td>\n",
       "      <td>Lời tòa soạn Thị trường bất động sản ngày càng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Việt Nam thành lập Viện nghiên cứu phát triển ...</td>\n",
       "      <td>Sáng 2/7, Đại học Quốc gia Thành phố Hồ Chí Mi...</td>\n",
       "      <td>Phát biểu tại buổi lễ, PGS.TS Huỳnh Thanh Đạt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>Bất động sản chờ lực bật mới</td>\n",
       "      <td>Chịu nhiều khó khăn do tác động từ đại dịch CO...</td>\n",
       "      <td>Nhu cầu thị trường nhà ở của người dân thuộc p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...</td>\n",
       "      <td>Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...</td>\n",
       "      <td>BVR&amp;MT – Năm 2020, Thị trường bất động sản có ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...</td>\n",
       "      <td>Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...</td>\n",
       "      <td>Đến ngày 29/6 tín dụng tăng 3,26%, là mức tăng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>Sẵn sàng đón nhận dòng vốn đầu tư dịch chuyển ...</td>\n",
       "      <td>Trong 6 tháng, Việt Nam cơ bản thực hiện thành...</td>\n",
       "      <td>Bộ trưởng Bộ Kế hoạch và Đầu tư Nguyễn Chí Dũn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>876 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "1    Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...   \n",
       "2    Đề xuất tăng liều lượng gói kích thích kinh tế...   \n",
       "3    Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "4    Tác động từ chính sách tín dụng: BĐS có thể đó...   \n",
       "..                                                 ...   \n",
       "871  Việt Nam thành lập Viện nghiên cứu phát triển ...   \n",
       "872                       Bất động sản chờ lực bật mới   \n",
       "873  Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...   \n",
       "874  Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...   \n",
       "875  Sẵn sàng đón nhận dòng vốn đầu tư dịch chuyển ...   \n",
       "\n",
       "                                               snippet  \\\n",
       "0    (Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...   \n",
       "1    Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...   \n",
       "2    Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...   \n",
       "3    Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...   \n",
       "4    Động thái thắt chặt hay nới lỏng của chính sác...   \n",
       "..                                                 ...   \n",
       "871  Sáng 2/7, Đại học Quốc gia Thành phố Hồ Chí Mi...   \n",
       "872  Chịu nhiều khó khăn do tác động từ đại dịch CO...   \n",
       "873  Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...   \n",
       "874  Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...   \n",
       "875  Trong 6 tháng, Việt Nam cơ bản thực hiện thành...   \n",
       "\n",
       "                                               content  \n",
       "0    Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...  \n",
       "1    Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...  \n",
       "2    Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...  \n",
       "3    Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...  \n",
       "4    Lời tòa soạn Thị trường bất động sản ngày càng...  \n",
       "..                                                 ...  \n",
       "871  Phát biểu tại buổi lễ, PGS.TS Huỳnh Thanh Đạt,...  \n",
       "872  Nhu cầu thị trường nhà ở của người dân thuộc p...  \n",
       "873  BVR&MT – Năm 2020, Thị trường bất động sản có ...  \n",
       "874  Đến ngày 29/6 tín dụng tăng 3,26%, là mức tăng...  \n",
       "875  Bộ trưởng Bộ Kế hoạch và Đầu tư Nguyễn Chí Dũn...  \n",
       "\n",
       "[876 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('Lecture_08/data.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "651749cb-7a5f-4843-b816-67b5a34effde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Làm sạch dữ liệu chỉ giữ lại dữ liệu bằng chữ\n",
    "# 1. Nếu là dữ liệu scrapped từ web thì chỉ trích lọc dữ liệu chứ (text only)\n",
    "# 2. Loại bỏ hyperlink (nếu có)\n",
    "# 3. Nếu là dữ liệu web thì cần loại bỏ emoji\n",
    "# 4. Loại bỏ tất cả các dấu (.,\\/!@#$%^&*()+_ etc.)\n",
    "# 5. Loại bỏ tất cả các số\n",
    "# 6. Loại bỏ các khoảng trắng và đổi text thành lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "703cf364-898e-4494-8640-a16d3cea1643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=76de452843188b89663e52ca63b724a1e3e836ae7a3642ee28f2d9930c46fd59\n",
      "  Stored in directory: c:\\users\\tdoan\\appdata\\local\\pip\\cache\\wheels\\75\\78\\21\\68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.9.3 bs4-0.0.1 soupsieve-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5959931b-a401-4a4b-9cc9-fd43e996ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "import bs4 \n",
    "\n",
    "def del_html(text):\n",
    "    soup = bs4.BeautifulSoup(text)\n",
    "    return soup.get_text(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62c1e331-1dcf-402b-8af8-b36629a639c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2.\n",
    "import re # regular expression\n",
    "\n",
    "def del_link(text):\n",
    "    link = r'http[\\S]*'\n",
    "    text = re.sub(link, ' ', str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4dee2e0-604d-4c64-ae3e-84cfb8eb648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting demoji\n",
      "  Downloading demoji-1.0.0-py3-none-any.whl (41 kB)\n",
      "Installing collected packages: demoji\n",
      "Successfully installed demoji-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04be7513-0bb3-4e8f-b7e6-3f517d5a8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "import demoji\n",
    "\n",
    "def del_emoji(text):\n",
    "    return demoji.replace(text, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b3915e9-e245-453d-9f89-ba460d5dbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "def del_punctuation(doc):\n",
    "    pattern = r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\“\\”\\\\\\-\\:…&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\•]'\n",
    "    record = re.sub(pattern, ' ', doc)\n",
    "    return re.sub(r'\\n', ' ', record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa243430-1dfd-4556-b5a8-5d762c3e4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.\n",
    "def del_numbers(text):\n",
    "    return re.sub(r'\\d+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4f2d156-59e4-4c3c-a266-f768aa33c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.\n",
    "def del_space(doc):\n",
    "    space_pattern = r'\\s+'\n",
    "    return re.sub(space_pattern, ' ', doc.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c281743-b3c3-4119-a0da-e99c8cc988fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn\n",
      "del_html : Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn\n",
      "del_link : Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn\n",
      "del_emoji : Thủ tướng: Khác với đa số các nước, dư địa chính sách tài khóa, tiền tệ của Việt Nam còn khá lớn\n",
      "del_punctuation : Thủ tướng  Khác với đa số các nước  dư địa chính sách tài khóa  tiền tệ của Việt Nam còn khá lớn\n",
      "del_numbers : Thủ tướng  Khác với đa số các nước  dư địa chính sách tài khóa  tiền tệ của Việt Nam còn khá lớn\n",
      "del_space : thủ tướng khác với đa số các nước dư địa chính sách tài khóa tiền tệ của việt nam còn khá lớn\n"
     ]
    }
   ],
   "source": [
    "text = data.iloc[0, 0]\n",
    "print(text)\n",
    "for func in [del_html, del_link, del_emoji, del_punctuation, \n",
    "             del_numbers, del_space]:\n",
    "    text = func(text)\n",
    "    print(func.__name__, ':', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c807e-c950-44b4-923a-3da9c08f74e8",
   "metadata": {},
   "source": [
    "#### Chia từ (Word tokenizing) - Tiếng Anh vs Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c985a9ba-9a19-4e02-9b09-e99c24578207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiếng Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3377d6c5-7b47-4a12-9dd4-9b5385f07928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.8.21-cp38-cp38-win_amd64.whl (270 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tdoan\\appdata\\roaming\\python\\python38\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.0.1 nltk-3.6.2 regex-2021.8.21\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "734b902e-accd-4b91-84ae-86700ca78c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7421b75-c2cf-407c-8506-a1ba5a31d44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog', '.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = 'A quick brown fox jump over a lazy dog.'\n",
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08e1184d-1a5f-4ec0-8621-cc3035cd3e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A quick brown fox jump over a lazy dog.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1e32561-efc2-4f71-9643-e2812fc0fcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'quick', 'brown', 'fox', 'jump', 'over', 'a', 'lazy', 'dog', '.'],\n",
       " ['He', 'is', 'the', 'royal', 'king', '.']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = '''A quick brown fox jump over a lazy dog.\n",
    "He is the royal king.\n",
    "'''\n",
    "\n",
    "[nltk.word_tokenize(i) for i in nltk.sent_tokenize(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a27ba149-2dfd-409f-a23f-550018530fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thủ', 'tướng', 'khác', 'với', 'đa', 'số', 'các', 'nước', 'dư', 'địa', 'chính', 'sách', 'tài', 'khóa', 'tiền', 'tệ', 'của', 'việt', 'nam', 'còn', 'khá', 'lớn']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea9033be-796a-42a6-a4ab-7fd5764c5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6fbfac9-9bab-4af3-b671-b64aad2476bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyvi) (0.24.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyvi) (1.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyvi) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyvi) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyvi) (1.19.5)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.62.1)\n",
      "Requirement already satisfied: six in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\tdoan\\appdata\\roaming\\python\\python38\\site-packages (from tqdm>=2.0->sklearn-crfsuite->pyvi) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd019cd1-8530-4639-8868-5159e79cd2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thủ_tướng khác với đa_số các nước dư địa_chính_sách tài khóa tiền_tệ của việt nam còn khá lớn'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi.ViTokenizer import tokenize\n",
    "\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bd0bb65-92a9-429a-8f4c-8c6c074f8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thủ_tướng', 'khác', 'với', 'đa_số', 'các', 'nước', 'dư', 'địa_chính_sách', 'tài', 'khóa', 'tiền_tệ', 'của', 'việt', 'nam', 'còn', 'khá', 'lớn']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(text).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9ba76-913e-4803-a39b-d6b537ed57bb",
   "metadata": {},
   "source": [
    "#### Put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c038b40f-907a-4cea-bd75-9ff06581b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = del_html(text)\n",
    "    text = del_link(text)\n",
    "    text = del_numbers(text)\n",
    "    text = del_emoji(text)\n",
    "    text = del_punctuation(text)\n",
    "    text = del_space(text)\n",
    "    return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98d08d23-2ebd-44e8-be76-6b7866777708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Thủ tướng: Khác với đa số các nước, dư địa chí...\n",
       "1      Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...\n",
       "2      Đề xuất tăng liều lượng gói kích thích kinh tế...\n",
       "3      Thủ tướng: Khác với đa số các nước, dư địa chí...\n",
       "4      Tác động từ chính sách tín dụng: BĐS có thể đó...\n",
       "                             ...                        \n",
       "871    Việt Nam thành lập Viện nghiên cứu phát triển ...\n",
       "872    Bất động sản chờ lực bật mới Chịu nhiều khó kh...\n",
       "873    Diễn đàn Bất động sản 2020: Cơ hội mới từ chín...\n",
       "874    Thống đốc Lê Minh Hưng: Tín dụng tăng trở lại,...\n",
       "875    Sẵn sàng đón nhận dòng vốn đầu tư dịch chuyển ...\n",
       "Name: corpus, Length: 876, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data.apply(lambda x: ' '.join(x), axis=1)\n",
    "data['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "664ac7b1-82d1-400c-bb94-0746b098a2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      thủ_tướng khác với đa_số các nước dư địa_chính...\n",
       "1      thống_nhất kịch_bản tăng_trưởng lạm_phát dưới ...\n",
       "2      đề_xuất tăng liều_lượng gói kích_thích kinh_tế...\n",
       "3      thủ_tướng khác với đa_số các nước dư địa_chính...\n",
       "4      tác_động từ chính_sách tín_dụng bđs có_thể đón...\n",
       "                             ...                        \n",
       "871    việt nam thành_lập viện nghiên_cứu phát_triển ...\n",
       "872    bất_động_sản chờ lực bật mới chịu nhiều khó_kh...\n",
       "873    diễn_đàn bất_động_sản cơ_hội mới từ chính_sách...\n",
       "874    thống_đốc lê minh hưng tín_dụng tăng trở_lại n...\n",
       "875    sẵn_sàng đón_nhận dòng vốn đầu_tư dịch_chuyển ...\n",
       "Name: corpus, Length: 876, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data['joined'].apply(clean_text)\n",
    "data['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df990f68-245c-41ed-8530-3c92e0947699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thủ_tướng khác với đa_số các nước dư địa_chính_sách tài khóa tiền_tệ của việt nam còn khá lớn tổ_quốc với bối_cảnh hiện_nay hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia thống_nhất kịch_bản tăng_trưởng từ kiểm_soát lạm_phát dưới năm và đầu tăng_trưởng tín_dụng trên chủ_trương tăng thêm bội_chi ngân_sách nợ công khoảng gdp sáng ngày hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia đã họp dưới sự chủ_trì của thủ_tướng nguyễn xuân phúc chủ_tịch hội_đồng dư địa_chính_sách tài khóa tiền_'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0, 'corpus'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa513f1e-0125-4cdc-a268-883af940025b",
   "metadata": {},
   "source": [
    "### Mã hóa dữ liệu text (Word embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d782c1-6302-4a8e-b8de-e936565beb95",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76b015ad-aa33-49a1-9937-677dffac05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển thành dạng lowercase (nếu chưa)\n",
    "# tách từ (tokenize), loại bỏ từ trùng lặp (set)\n",
    "# Sắp xếp theo thứ tự a-z\n",
    "# Lấy giá trị vị trí\n",
    "# Chuyển đổi thành vector [0, 1] tương ứng giá trị vị trí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e93876c-d23f-4e3b-b6f0-8e21ee77c6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'eat', 'i', 'pizza', 'the']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Can I eat the pizza'\n",
    "sorted(nltk.word_tokenize(sentence.lower()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97cd81d8-3d7c-4aa6-86b9-45cdfa7eb8e1",
   "metadata": {},
   "source": [
    "[[1. 0. 0. 0. 0.] # can\n",
    " [0. 0. 1. 0. 0.] # i\n",
    " [0. 1. 0. 0. 0.] # eat\n",
    " [0. 0. 0. 0. 1.] # the\n",
    " [0. 0. 0. 1. 0.]] # pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b38888df-bbca-4618-9546-b49bc1be16e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "values:  ['can', 'eat', 'i', 'pizza', 'the']\n",
      "\n",
      "integer encoded:  [0, 2, 1, 4, 3]\n",
      "\n",
      "MATRIX:\n",
      "[[1 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "docs = \"Can I eat the Pizza\".lower().split()\n",
    "doc1 = set(docs)\n",
    "doc1 = sorted(doc1)\n",
    "print (\"\\nvalues: \", doc1)\n",
    "\n",
    "integer_encoded = []\n",
    "for i in docs:\n",
    "    v = np.where( np.array(doc1) == i)[0][0]\n",
    "    integer_encoded.append(v)\n",
    "print (\"\\ninteger encoded: \",integer_encoded)\n",
    "\n",
    "def get_vec(len_doc,word):\n",
    "    empty_vector = [0] * len_doc\n",
    "    vect = 0\n",
    "    find = np.where( np.array(doc1) == word)[0][0]\n",
    "    empty_vector[find] = 1\n",
    "    return empty_vector\n",
    "\n",
    "def get_matrix(doc1):\n",
    "    mat = []\n",
    "    len_doc = len(doc1)\n",
    "    for i in docs:\n",
    "        vec = get_vec(len_doc,i)\n",
    "        mat.append(vec)\n",
    "        \n",
    "    return np.asarray(mat)\n",
    "\n",
    "print (\"\\nMATRIX:\")\n",
    "print (get_matrix(doc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b347065-59af-414d-8fee-f37389be5b5d",
   "metadata": {},
   "source": [
    "### Khai phá dữ liệu text (Text Mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82a86e-d2e8-4c4c-a418-124c90218d1f",
   "metadata": {},
   "source": [
    "#### Khái niệm Bag-of-Words\n",
    "\n",
    "Là kỹ thuật chia văn bản thành các tổ hợp từ khác nhau (bằng phương pháp tokenize). Cách chia phổ biến là mỗi câu thành 1 văn bản (bag) và mỗi văn bản được chia thành từ (word). Dựa vào đó có thể đo lường mức độ xuất hiện của các từ trong văn bản và xây dựng mối liên hệ giữa ngữ cảnh và các từ. \n",
    "\n",
    "Hai yếu tố:\n",
    "1. Từ điển của các từ được sử dụng\n",
    "2. Mức độ xuất hiện của các từ trong từ điển\n",
    "*Mỗi từ hay token được gọi là một `gram`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a2f157ea-c709-404b-a67c-fb87442d5f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was the best of times',\n",
       " 'It was the worst of times',\n",
       " 'It was the age of wisdom',\n",
       " 'It was the age of foolishness']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'It was the best of times',\n",
    "    'It was the worst of times',\n",
    "    'It was the age of wisdom',\n",
    "    'It was the age of foolishness'\n",
    "]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dbea88b0-c557-4e32-92bf-d9ff4d024937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', 'worst', 'age', 'wisdom', 'foolishness']\n"
     ]
    }
   ],
   "source": [
    "dictionary = []\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    for w in words:\n",
    "        if w not in dictionary:\n",
    "            dictionary.append(w)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "125e3c08-234f-4810-a497-c50e7c531ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times :\t [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "It was the worst of times :\t [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "It was the age of wisdom :\t [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "It was the age of foolishness :\t [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    vec = [1 if w in sent else 0 for w in dictionary]\n",
    "    print(sent, ':\\t', vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term frequency-Inverse Document frequency)\n",
    "\n",
    "Đo lường tần suất ***hợp lý*** một từ (hay token) xuất hiện trong văn bản. Tần suất này được tính bằng: Mức độ xuất hiện của từ trong văn bản chia cho tỉ lệ văn bản mà từ đó xuất hiện trên tổng tất cả số lượng văn bản.\n",
    "\n",
    "Ví dụ: đối với như `what` hay `the`, các từ này xuất hiện rất nhiều tuy nhiên ko mang nhiều ý nghĩa nên cần có phương pháp loại trừ các từ này ra khỏi mô hình. Vì vậy ngoài tính toán mức độ xuất hiện của các từ trong văn bản, tuy nhiên nếu văn bản nào cũng xuất hiện từ này (hoặc đơn giản là rất nhiều > 90%) thì các từ này sẽ bị loại ra.\n",
    "\n",
    "$$ tf-idf(t, d, D)  = tf(t, d) \\dot idf(t, D)$$\n",
    "\n",
    "> `t`: từ (word hay token)\n",
    " \n",
    "> `d`: văn bản (document)\n",
    " \n",
    "> `D`: tệp các văn bản (documents)\n",
    "\n",
    "trong đó:\n",
    "\n",
    "$$ tf(t, d) = \\log(1 + freq(t, d)) $$\n",
    "$$ idf(t, D) = \\log \\left( \\dfrac{N}{count(d \\in D: t \\in d)} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d500e30-be68-45fe-8fcb-296cc4c460cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "Successfully installed Cython-0.29.21 gensim-4.0.1 smart-open-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97fd43b9-a214-460b-96a2-a7e525f0df43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['age', 1], ['foolishness', 1], ['it', 1], ['of', 1], ['the', 1], ['was', 1]]\n",
      "[['age', 1], ['foolishness', 1], ['it', 1], ['of', 1], ['the', 1], ['was', 1]]\n",
      "[['age', 1], ['foolishness', 1], ['it', 1], ['of', 1], ['the', 1], ['was', 1]]\n",
      "[['age', 1], ['foolishness', 1], ['it', 1], ['of', 1], ['the', 1], ['was', 1]]\n",
      "[['age', 0.0], ['foolishness', 0.0], ['it', 0.0], ['of', 0.0], ['the', 0.0], ['was', 0.0]]\n",
      "[['age', 0.0], ['foolishness', 0.0], ['it', 0.0], ['of', 0.0], ['the', 0.0], ['was', 0.0]]\n",
      "[['age', 0.0], ['foolishness', 0.0], ['it', 0.0], ['of', 0.0], ['the', 0.0], ['was', 0.0]]\n",
      "[['age', 0.0], ['foolishness', 0.0], ['it', 0.0], ['of', 0.0], ['the', 0.0], ['was', 0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tdoan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "tokenized = [simple_preprocess(sent) for doc in sentences]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "\n",
    "for doc in BoW_corpus:\n",
    "    print([[dictionary[id], freq] for id, freq in doc])\n",
    "\n",
    "tfidf = models.TfidfModel(BoW_corpus, smartirs='ntc')\n",
    "for doc in tfidf[BoW_corpus]:\n",
    "    print([[dictionary[id], np.around(freq)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b11b6e9-3271-4c0b-b9b8-51709f57ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 'was', 'the', 'best', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'worst', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'wisdom'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'foolishness']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = [simple_preprocess(sent) for sent in sentences]\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb319f69-70ac-4e82-ac01-ded493dc4e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (2, 1), (3, 1), (5, 1), (7, 1), (9, 1)]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary()\n",
    "bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenized]\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d37112c-a377-4e54-a479-749d95fb1d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['best', 1], ['it', 1], ['of', 1], ['the', 1], ['times', 1], ['was', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['times', 1], ['was', 1], ['worst', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['was', 1], ['age', 1], ['wisdom', 1]]\n",
      "[['it', 1], ['of', 1], ['the', 1], ['was', 1], ['age', 1], ['foolishness', 1]]\n"
     ]
    }
   ],
   "source": [
    "for doc in bow_corpus:\n",
    "    print([[dictionary[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c84a2c4c-d198-4445-8b01-9b12c23990f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x265f2fbd700>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus, smartirs='ntc')\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "25d55915-d15d-46b4-98a7-878a8d61118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['best', 0.8448462391634638], ['it', 0.11713529839512135], ['of', 0.11713529839512135], ['the', 0.11713529839512135], ['times', 0.48099076877929264], ['was', 0.11713529839512135]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['times', 0.48099076877929253], ['was', 0.11713529839512132], ['worst', 0.8448462391634637]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['was', 0.11713529839512132], ['age', 0.48099076877929253], ['wisdom', 0.8448462391634637]]\n",
      "[['it', 0.11713529839512132], ['of', 0.11713529839512132], ['the', 0.11713529839512132], ['was', 0.11713529839512132], ['age', 0.48099076877929253], ['foolishness', 0.8448462391634637]]\n"
     ]
    }
   ],
   "source": [
    "for doc in tfidf[bow_corpus]:\n",
    "    print([[dictionary[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ada12a94-82d2-4973-8041-1e8583a55584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['biện_pháp', 0.0412356814796833],\n",
       " ['bán_lẻ', 0.04929934590749365],\n",
       " ['bên', 0.022246915075162407],\n",
       " ['bùi', 0.0714924798804455],\n",
       " ['bùng_phát', 0.04571307724402378],\n",
       " ['bảo_vệ', 0.053548195600883426],\n",
       " ['bối_cảnh', 0.03481354147648335],\n",
       " ['bội_chi', 0.10106491002082081],\n",
       " ['ca', 0.06087278261510201],\n",
       " ['chi', 0.04929934590749365]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary()\n",
    "doc_tokenized = [doc.split(' ') for doc in data['corpus']]\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "\n",
    "model = models.TfidfModel(BoW_corpus)\n",
    "[[dictionary[id], freq] for id, freq in model[BoW_corpus[0]][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc2036-7dc6-4b33-baac-7c1ab14fe285",
   "metadata": {},
   "source": [
    "#### Mô hình Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f5ac5-3024-4991-8a7b-27ec8002ac29",
   "metadata": {},
   "source": [
    "*\"Word2Vec was developed at Google by Tomas Mikolov, et al. and uses Neural Networks to learn word embeddings. The beauty with word2vec is that the vectors are learned by understanding the context in which words appear. The result is vectors in which words with similar meanings end up with a similar numerical representation.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe00fd3-483f-4343-b331-8bdbde4d8d03",
   "metadata": {},
   "source": [
    "**One-hot-encoding**\n",
    "\n",
    "All word are treated equal\n",
    "\n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/b3c56245-db43-48ab-b652-9ba03f4d9900.jpg?ssl=1)\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "Word with similar numeric value are similar in meaning\n",
    "\n",
    "![](https://i1.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/8cbfc874-3ba3-46c8-ab68-2711812ecbf1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e7484-2125-4bfa-8033-9f11341f6c93",
   "metadata": {},
   "source": [
    "Hai loại mô hình Word2Vec: **CBOW** (Continuous Bag-of-Word) và **Skip-Gram**\n",
    "\n",
    "Continuous Bag of Words (CBOW): *nhìn hình (ngữ cảnh) đoán chữ*\n",
    "\n",
    "Ngược lại, Skip-Gram: *nhìn chữ đoán hình (ngữ cảnh)*\n",
    "\n",
    "![](https://i0.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/7938152f-71c8-4f28-9c25-06735e6e2b67.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e4d21d0-2f49-4f45-a365-5f0c2718df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "41c90846-6e81-448f-b604-c797cab58b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(doc_tokenized, min_count=5, sg=0) # 1 == skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e0289002-5b51-4884-8bb6-8cbaf18f8cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "các                           0\n",
      "và                            1\n",
      "trong                         2\n",
      "kinh_tế                       3\n",
      "của                           4\n",
      "                           ... \n",
      "thitruongtaichinhtiente    3411\n",
      "giải_đáp                   3412\n",
      "mùi                        3413\n",
      "kịch_bản_chính_phủ         3414\n",
      "đòn_bẩy                    3415\n",
      "Length: 3416, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.key_to_index\n",
    "print(pd.Series(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f4bdfb20-169b-46c0-833f-0c3e1859d1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.82342157e-01,  3.83873552e-01, -1.44534394e-01,  7.65922964e-02,\n",
       "        3.79345298e-01, -8.65793601e-02,  6.54608086e-02,  3.48696619e-01,\n",
       "       -7.62954503e-02,  2.48731732e-01, -2.99366474e-01,  1.77202493e-01,\n",
       "        1.85716361e-01,  3.42042819e-02,  3.39961052e-02,  3.26030195e-01,\n",
       "        2.39822686e-01, -1.57269493e-01,  1.29891410e-01, -7.24578351e-02,\n",
       "       -3.46942693e-02,  1.01858690e-01,  1.10700741e-01, -3.27607900e-01,\n",
       "       -1.59938946e-01,  1.40665993e-01, -7.28769675e-02, -9.45709646e-02,\n",
       "       -8.08669925e-02,  2.94243246e-01,  2.58075923e-01,  7.69154653e-02,\n",
       "       -1.48420945e-01, -5.89549877e-02, -1.42466366e-01,  1.82856530e-01,\n",
       "       -1.44799575e-01, -3.64023000e-02, -1.14192717e-01, -2.64315218e-01,\n",
       "        1.23347327e-01, -2.37349595e-05,  2.43588090e-01, -1.66029021e-01,\n",
       "       -2.81853884e-01,  4.55741175e-02, -2.96600282e-01, -1.35715514e-01,\n",
       "       -1.56706020e-01, -4.54942659e-02,  1.19437270e-01,  1.32989526e-01,\n",
       "        7.24774823e-02, -2.55789965e-01, -9.50843915e-02, -2.47667626e-01,\n",
       "       -1.61140844e-01,  2.71283627e-01, -6.69685602e-01,  1.97404623e-02,\n",
       "        4.13988769e-01, -1.12049215e-01,  1.65302619e-01,  2.76317030e-01,\n",
       "        1.69475134e-02, -8.37212577e-02, -7.32527599e-02,  3.93064350e-01,\n",
       "        7.59975985e-02,  3.17006111e-01, -4.55067158e-01, -1.18850820e-01,\n",
       "       -7.61073008e-02, -1.96115881e-01,  2.87437379e-01,  3.74801606e-02,\n",
       "       -2.32257754e-01,  7.63640776e-02,  5.65267280e-02,  1.83460474e-01,\n",
       "       -7.67432600e-02,  5.85832261e-02, -4.81503040e-01,  1.49581417e-01,\n",
       "       -2.16481313e-01,  5.14062718e-02,  1.01982526e-01,  1.46928564e-01,\n",
       "        2.12381899e-01, -1.08522534e-01,  3.18289191e-01, -2.68378437e-01,\n",
       "       -4.69963431e-01, -2.83636134e-02,  9.95045826e-02, -4.55987342e-02,\n",
       "        1.46571994e-01, -1.63994715e-01, -9.11567211e-02,  3.33293248e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = word2vec.wv['tổ_quốc']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "245edc78-bd0b-4642-b508-20c65552ea40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ngắn', 0.6528961658477783),\n",
       " ('điểm_nóng', 0.5696249008178711),\n",
       " ('khiếu_kiện', 0.5383105278015137),\n",
       " ('cứu', 0.5162107348442078),\n",
       " ('kết_thúc', 0.4972623288631439),\n",
       " ('nó', 0.48638781905174255),\n",
       " ('mạnh_tay', 0.4734785854816437),\n",
       " ('đối_phó', 0.4685332179069519),\n",
       " ('khó', 0.4460851550102234),\n",
       " ('tồn_tại', 0.44549649953842163)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words = word2vec.wv.most_similar('kéo_dài')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aea233-3d03-4566-8876-153325d71cd9",
   "metadata": {},
   "source": [
    "**Skip-gram**\n",
    "\n",
    "Window Size defines how many words before and after the target word will be used as context, typically a Window Size is 5. \n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/a8066c1d-c532-4549-bb24-19dfea5eb178_med.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860004a0-d665-4d69-a4b6-41f7ef94ccea",
   "metadata": {},
   "source": [
    "Using a window size of 2 the input pairs for training on w(4) royal would be:\n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/training_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f63ee0db-98c0-4baa-84b4-9a08b988bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(doc_tokenized, min_count=5, sg=1) # 1 == skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d07e184d-7c60-4ed9-9f52-11fb709641d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "các                           0\n",
      "và                            1\n",
      "trong                         2\n",
      "kinh_tế                       3\n",
      "của                           4\n",
      "                           ... \n",
      "thitruongtaichinhtiente    3411\n",
      "giải_đáp                   3412\n",
      "mùi                        3413\n",
      "kịch_bản_chính_phủ         3414\n",
      "đòn_bẩy                    3415\n",
      "Length: 3416, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.key_to_index\n",
    "print(pd.Series(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "63630684-fb57-41cc-bf89-ccb47f16e5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16557786,  0.5547214 , -0.0700129 ,  0.1856907 ,  0.31796294,\n",
       "       -0.10810167, -0.23590812,  0.2633175 , -0.08135381,  0.20161693,\n",
       "       -0.28856915,  0.14334552, -0.01961311, -0.00207503,  0.30414698,\n",
       "        0.40060127,  0.13037017, -0.34005964,  0.09793479, -0.07910369,\n",
       "       -0.01611584, -0.02239303,  0.13974565, -0.17828597, -0.05571358,\n",
       "        0.1735891 , -0.05605118, -0.02122932, -0.00363449,  0.11936511,\n",
       "        0.06295639,  0.2585639 , -0.19181433,  0.15629879, -0.16519181,\n",
       "        0.5765332 , -0.25817135, -0.3060833 , -0.014099  , -0.40463704,\n",
       "        0.07919086, -0.04855295,  0.51362985,  0.04634253, -0.34635028,\n",
       "        0.23560238, -0.16420652, -0.24368556, -0.2738499 , -0.03009408,\n",
       "        0.14965206, -0.3480504 ,  0.16400012, -0.07400417, -0.13233477,\n",
       "       -0.3096474 , -0.0051062 ,  0.2159299 , -0.4862298 , -0.29382944,\n",
       "        0.6075605 ,  0.00283417,  0.05742108,  0.24343829, -0.10527484,\n",
       "       -0.22211182, -0.02327074,  0.1256666 ,  0.17983614,  0.13360211,\n",
       "       -0.34055802, -0.26068065, -0.30947012,  0.09987548,  0.3325695 ,\n",
       "        0.3130551 , -0.14847937,  0.09651107, -0.2360515 ,  0.05573852,\n",
       "       -0.13906066,  0.31894505, -0.3788161 ,  0.2507681 ,  0.05117008,\n",
       "       -0.3028256 , -0.09859905,  0.19112441,  0.0179533 , -0.04410019,\n",
       "        0.20987959, -0.35141975, -0.36947048, -0.00091215,  0.2338743 ,\n",
       "        0.17269334,  0.2832178 , -0.05766806,  0.06342208, -0.14584813],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = word2vec.wv['tổ_quốc']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "75f1823d-a501-4b76-87c3-45e174b30ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ngắn', 0.6494366526603699),\n",
       " ('khiếu_kiện', 0.6469146013259888),\n",
       " ('công_cuộc', 0.6077184677124023),\n",
       " ('điểm_nóng', 0.5989739298820496),\n",
       " ('tiêu_tan', 0.5805805325508118),\n",
       " ('tạm_ứng', 0.5127071142196655),\n",
       " ('phát_huy_hiệu_quả', 0.5120418667793274),\n",
       " ('chăn', 0.506695568561554),\n",
       " ('dẫu', 0.5055784583091736),\n",
       " ('chờ_đợi', 0.5048704147338867)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words = word2vec.wv.most_similar('kéo_dài')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9161eac-d4a4-499f-a291-150bb9bbdab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
